{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyN6uqewOPomdsX00/81WqEy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9PygRWMNLOLI"},"outputs":[],"source":["!pip install -q groq"]},{"cell_type":"code","source":["import os\n","from groq import Groq"],"metadata":{"id":"xbeLDDo0LREA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up Groq client with API key\n","client = Groq(\n","    api_key=\"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\"  # Or directly input your API key here\n",")"],"metadata":{"id":"_ylHEDYVLRG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_response(user_query):\n","    \"\"\"\n","    Generate a response for any user query using Groq's API, with no hard-coding of topics.\n","    The model is expected to handle diverse topics dynamically.\n","    \"\"\"\n","    # System message that instructs the model to answer any query across various topics\n","    system_message = (\n","        \"You are a helpful assistant capable of answering questions on a wide range of topics, \"\n","        \"including programming, history, science, general knowledge, mathematics, education, and more. \"\n","        \"Provide clear, concise, and accurate answers to any question the user asks.\"\n","    )\n","\n","    # User's question\n","    user_message = user_query\n","\n","    # Create the chat completion using Groq API\n","    chat_completion = client.chat.completions.create(\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_message},\n","            {\"role\": \"user\", \"content\": user_message}\n","        ],\n","        model=\"mixtral-8x7b-32768\"  # Use the appropriate model for general use\n","    )\n","\n","    # Return the response content\n","    return chat_completion.choices[0].message.content\n"],"metadata":{"id":"bprDRJFwLRJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Programming-related query\n","programming_query = \"What is Backward propogation in Neural networks and how it works step by step?\"\n","print(\"User:\", programming_query)\n","print(\"AI:\", generate_response(programming_query))"],"metadata":{"id":"KIWts8-KLRMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def interactive_chat():\n","    \"\"\"\n","    Start an interactive chat session where the user can ask any question across a variety of topics.\n","    \"\"\"\n","    print(\"Welcome! You can ask any question, and I will provide the answer. Type 'exit' to end the conversation.\\n\")\n","\n","    while True:\n","        # Get user input\n","        user_input = input(\"You: \")\n","\n","        # Check if the user wants to exit the conversation\n","        if user_input.lower() in ['exit', 'quit']:\n","            print(\"Exiting the chat...\")\n","            break\n","\n","        # Get the AI's response for the user's query\n","        response = generate_response(user_input)\n","        print(f\"AI: {response}\")\n","\n","# Start the interactive chat\n","interactive_chat()"],"metadata":{"id":"3HZFSwOPLRPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"18_EACGcLRSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0P7ev0fOLRXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JwGQOE5pLRaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HLGHiW78LRfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# memory updated.........."],"metadata":{"id":"mQFOrAARRM7X"}},{"cell_type":"code","source":["!pip install -q groq"],"metadata":{"id":"zEzCC4fWLRip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from groq import Groq\n","import json"],"metadata":{"id":"2_YoerLMLRl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up Groq client with API key\n","client = Groq(\n","    api_key=\"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\"  # Or directly input your API key here\n",")"],"metadata":{"id":"BiajmzRsRMW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize conversation history with instructions to provide concise and clear answers\n","conversation_history = [\n","    {\"role\": \"system\", \"content\":\n","     \"You are a highly capable and helpful assistant, trained to answer a wide variety of questions on various topics. \"\n","     \"Your main goal is to assist users by providing clear, concise, and accurate answers. \"\n","     \"When responding to a user's query, keep your answers short, focused, and easy to understand, without unnecessary details. \"\n","     \"Only provide more detailed explanations if the user specifically requests more information (e.g., 'explain in more detail', 'elaborate', 'provide further explanation'). \"\n","     \"If the user is asking a complex question, break down your response into manageable chunks and provide step-by-step explanations where needed. \"\n","     \"Be polite and encouraging, and make sure to provide accurate information at all times. \"\n","     \"You should also remember all previous exchanges in the conversation and use that information to provide better context and relevant responses.\"\n","    }\n","]"],"metadata":{"id":"2GN5Y04YRMIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_response_with_memory(user_query):\n","    \"\"\"\n","    Generate a response for any user query, while keeping track of the conversation history.\n","    The model will remember previous exchanges for context.\n","    \"\"\"\n","    # Add the user's message to the conversation history\n","    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n","\n","    # Create the chat completion using Groq API with the updated conversation history\n","    chat_completion = client.chat.completions.create(\n","        messages=conversation_history,\n","        model=\"mixtral-8x7b-32768\"  # Use the appropriate model for general use\n","    )\n","\n","    # Get the AI's response\n","    ai_response = chat_completion.choices[0].message.content\n","\n","    # Add the AI's response to the conversation history\n","    conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n","\n","    # Return the response\n","    return ai_response"],"metadata":{"id":"2TT7jk1iRMFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def interactive_chat_with_memory():\n","    \"\"\"\n","    Start an interactive chat session where the user can ask any question and the model remembers previous exchanges.\n","    \"\"\"\n","    print(\"Welcome! You can ask any question, and I will provide the answer. I remember everything you ask!\")\n","    print(\"Type 'exit' to end the conversation.\\n\")\n","\n","    while True:\n","        # Get user input\n","        print(\"\")\n","        user_input = input(\"You: \")\n","\n","        # Check if the user wants to exit the conversation\n","        if user_input.lower() in ['exit', 'quit']:\n","            print(\"Exiting the chat...\")\n","            break\n","\n","        # Get the AI's response for the user's query, using memory\n","        response = generate_response_with_memory(user_input)\n","        print(f\"AI: {response}\")\n","\n","# Start the interactive chat with memory\n","interactive_chat_with_memory()"],"metadata":{"id":"O3WitV_qRMDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4qLfjsJ1RL-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U4YZprbSRL7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y_c5mWbJRL4_"},"execution_count":null,"outputs":[]}]}